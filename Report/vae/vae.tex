\documentclass[../report.tex]{subfiles}
\begin{document}
The Variational Autoencoder exploits the methods described in the previous section to define a probabilistic model whose elbo is highly reminiscent of the objective optimised in a traditional autoencoder. In particular we define a generative model where we assume that the $i^{th}$ observation was generated by first sampling a latent variable $z_i \sim \dNorm{0, I}$ and that the each observation $x_i \sim \dNorm{\Func{\mu_\theta}{z_i}, \Func{\sigma^2_\theta}{z_i}}$ if the observations are real-valued, or $x_i \sim \dBern{\Func{f_\theta}{z_i}}$ if they are binary or valued on $[0, 1]$. In both cases the distribution parameters $\Func{\mu_\theta}{z_i}, \Func{\sigma^2_\theta}{z_i}, \Func{f_\theta}{z_i}$ are parameterised in terms of a multi-layer perceptron (MLP) whose input is $z_i$, which will be referred to as the ``decoder MLP''. Specifically, define $h_i$ to be the vector output at the final hidden layer of the decoder MLP when provided input $z_i$, then
\begin{align}
  \Func{\mu_\theta}{z_i} =& h_i W_\mu^{(q)} + b_\mu^{(q)}, \\
  \log \Func{\sigma^2_\theta}{z_i} =& h_i W_\sigma^{(q)} + b_\sigma^{(q)}.
\end{align}





The recognition model is given by
\begin{equation}
  \CondFunc{q_\phi}{z}{x} = \pdfNorm{z}{\Func{\mu_\phi}{x_i}, \Func{\sigma_\phi^2}{x_i}},
\end{equation}
where the distributional parameters $\Func{\mu_\phi}{x_i}$ and $\Func{\sigma_\phi^2}{x_i}$ are again given by an MLP whose input is $x_i$. Note that whenever a variance is parameterised by an MLP, 

\end{document}