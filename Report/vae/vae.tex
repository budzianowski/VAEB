\documentclass[../report.tex]{subfiles}
\begin{document}
The Variational Autoencoder exploits the methods described in the previous section to define a probabilistic model whose elbo is highly reminiscent of the objective optimised in a traditional autoencoder. In particular we define a generative model where we assume that the $i^{th}$ observation was generated by first sampling a latent variable $\mathbf{z}_i \sim \dNorm{0, I}$ and that the each observation vector real-valued observation $\mathbf{x}_i \sim \dNorm{\Func{\mu_\theta}{\mathbf{z}_i}, \Func{\sigma^2_\theta}{\mathbf{z}_i}}$ where
\begin{align}
  \Func{\mathbf{\mu}_\theta}{\mathbf{z}_i} =& \mathbf{h}_i W_\mathbf{\mu}^{(dec)} + \mathbf{b}_\mathbf{\mu}^{(dec)}, \\
  \log \Func{\sigma^2_\theta}{\mathbf{z}_i} =& \mathbf{h}_i W_\sigma^{(dec)} + \mathbf{b}_\sigma^{(dec)},
\end{align}
$\mathbf{h}_i \in \mathbb{R}^{1 \times D_h}$ is the output at the final hidden layer of the ``decoder MLP'', $W_\mu^{(dec)}, W_\sigma^{(dec)} \in \mathbb{R}^{D_h \times D_z}$ are matrices mapping from the $D_h$ hidden units to the $D_z$ dimensional latent space. Similarly, $\mathbf{b}_\mu^{(dec)}, \mathbf{b}_\sigma^{(dec)} \in \mathbb{R}^{1 \times D_z}$ are row vector biases. Note that the variances are parameterised implicitely through their logs to ensure that they are correctly valued only on the positive reals.

If the output vectors are binary valued then $x_i \sim \dBern{\Func{f_\theta}{\mathbf{z}_i}}$ where again given $\mathbf{h}_i$, the output at the final hidden layer of the decoder MLP for latent-space value $\mathbf{z}_i$
\begin{equation}
  \Func{f_\theta}{\mathbf{z}_i} = \left[1 + \Func{\exp}{- \mathbf{h}_i W^{(dec)} - \mathbf{b}^{(dec)}} \right]^{-1},
\end{equation}
where $W^{(dec)} \in \mathbb{R}^{D_h \times D_z}$ and $b^{(dec)} \in \mathbb{R}^{1 \times D_z}$.

The recognition model is given by
\begin{equation}
  \CondFunc{q_\phi}{\mathbf{z}}{\mathbf{x}} = \pdfNorm{\mathbf{z}}{\Func{\mu_\phi}{\mathbf{x}_i}, \Func{\sigma_\phi^2}{\mathbf{x}_i}},
\end{equation}
where the distributional parameters $\Func{\mu_\phi}{\mathbf{x}_i}$ and $\Func{\sigma_\phi^2}{\mathbf{x}_i}$ are again given by an MLP, which will be refered to as the ``encoding MLP'', whose input is $\mathbf{x}_i$.

In this case, there is a tractable closed form solution for the KL-divergence between the variational posterior $\CondFunc{q_\phi}{\mathbf{z}}{\mathbf{x}}$ and the prior $\Func{p_\theta}{\mathbf{z}}$ meaning that we can approximate the ELBO using equation \ref{eqn:Lb}. Additionally we have that
\begin{equation}
  \Func{g_\phi}{\mathbf{x}, \epsilon} = \Func{\mu_\phi}{\mathbf{x}_i} + \Func{\sigma_\phi}{\mathbf{x}_i} \odot \epsilon, \quad \epsilon \sim \dNorm{0, I}.
\end{equation}

The algorithm to perform inference in the Variational Autoencoder is provided in algorithm \ref{alg:vae}.

\begin{algorithm}[!bthp]
\caption{Procedure by which inference is performed in the Variational Autoencoder.}
\label{alg:vae}
\begin{algorithmic}
\State $\theta, \phi \gets \textrm{Initialisation}.$
\While {$\textrm{not converged in } \theta, \phi$}
  \State {Pick subset of size $\mathbf{x}_{1:M}$ from the full dataset uniformly at random.}
  \State {Compute $\mu_{1:M}, \log \sigma_{1:M}^2$ using the encoding MLP. }
  \State {For all $i \in \{1,...,M\}$, sample $\epsilon_{i} \sim\dNorm{0, I}$.}
  \State {For all $i \in \{1,...,M\}$, $\mathbf{z}_i \gets \mu_i + \sigma_i \odot \epsilon_i$.}
  \State {For all $i \in \{1,...,M\}$, compute $\log \Cond{\mathbf{x}_i}{\mathbf{z}_i}$ using the decoder MLP and likelihood function.}
  \State {Compute $g \gets \nabla_{\theta, \phi} \Func{\tilde{\mathcal{L}}^B}{\theta, \phi; \mathbf{x}_{1:M}, \epsilon_{1:M}}$}
  \State {Update $\theta, \phi$ using noisy gradient estimate $g$.}
\EndWhile
\end{algorithmic}
\end{algorithm}

\end{document}