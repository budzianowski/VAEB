\documentclass[../report.tex]{subfiles}
\begin{document}
The Variational Autoencoder exploits the methods described in the previous section to define a probabilistic model whose elbo is highly reminiscent of the objective optimised in a traditional autoencoder. In particular we define a generative model where we assume that the $i^{th}$ observation was generated by first sampling a latent variable $z_i \sim \dNorm{0, I}$ and that the each observation vector real-valued observation $x_i \sim \dNorm{\Func{\mu_\theta}{z_i}, \Func{\sigma^2_\theta}{z_i}}$ where
\begin{align}
  \Func{\mu_\theta}{z_i} =& h_i W_\mu^{(q)} + b_\mu^{(q)}, \\
  \log \Func{\sigma^2_\theta}{z_i} =& h_i W_\sigma^{(q)} + b_\sigma^{(q)},
\end{align}
$h_i \in \mathbb{R}^{1 \times D_h}$ is the output at the final hidden layer of the ``decoder MLP'', $W_\mu^{(q)}, W_\sigma^{(q)} \in \mathbb{R}^{D_h \times D_z}$ are matrices mapping from the $D_h$ hidden units to the $D_z$ dimensional latent space. Similarly, $b_\mu^{(q)}, b_\sigma^{(q)} \in \mathbb{R}^{1 \times D_z}$ are row vector biases. Note that the variances are parameterised implicitely through their logs to ensure that they are correctly valued only on the positive reals.

If the output vectors are to be treated as either probabilities or are binary values then $x_i \sim \dBern{\Func{f_\theta}{z_i}}$ where again given $h_i$, the output at the final hidden layer of the decoder MLP for latent-space value $z_i$
\begin{equation}
  \Func{f_\theta}{z_i} = \left[1 + \Func{\exp}{- h_i W^{(q)} - b^{(q)}} \right]^{-1},
\end{equation}
where $W^{(q)} \in \mathbb{R}^{D_h \times D_z}$ and $b^{(q)} \in \mathbb{R}^{1 \times D_z}$.


%or $x_i \sim  if they are binary or valued on $[0, 1]$. In both cases the distribution parameters $\Func{\mu_\theta}{z_i}, \Func{\sigma^2_\theta}{z_i}, \Func{f_\theta}{z_i}$ are parameterised in terms of a multi-layer perceptron (MLP) whose input is $z_i$, which will be referred to as the ``decoder MLP''. Specifically, define $h_i$ to be the vector output at the final hidden layer of the decoder MLP when provided input $z_i$, then






The recognition model is given by
\begin{equation}
  \CondFunc{q_\phi}{z}{x} = \pdfNorm{z}{\Func{\mu_\phi}{x_i}, \Func{\sigma_\phi^2}{x_i}},
\end{equation}
where the distributional parameters $\Func{\mu_\phi}{x_i}$ and $\Func{\sigma_\phi^2}{x_i}$ are again given by an MLP whose input is $x_i$. Note that whenever a variance is parameterised by an MLP, 

\end{document}