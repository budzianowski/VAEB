\documentclass[../report.tex]{subfiles}
\begin{document}
\subsection{Latent space priors}
An obvious extension to the paper to investigate is to simply change the form of the prior and the variational approximation in an attempt to induce a particular form of latent space. For example a particularly interesting set up would be to define a sparsity inducing prior that encourages each dimension of the latent space to be approximately valued on $\{0, 1\}$. An obvious choice would be a set of sparse Beta distributions (ie. ones in which the shape parameters $\alpha, \beta < 1$), but one could also use pairs of univariate Gaussians with means $0$ and $1$ and small variances.

Such a prior would be useful for two reasons - firstly it would allow one to provide a binary encoding for a data set by truncating the posterior approximation for any particular observation to be exactly vector binary valued allowing for a large amount of lossy compression. The posterior distribution over the parameters $\theta$ and latent values $\mathbf{z}_i$ also contains rotational symmetry which may affect the quality of the approximate inference if it attempts to place posterior mass over the entirety of this. Were a prior such as the one proposed used, this rotational symmetry would be destroyed and replaced with a ``permutation symmetry'', similar to that found in a finite mixture model.
\\
\subsection{Non-parametric posterior approximation}
We currently assume a simple parametric form for the approximate posterior $q_\phi(\mathbf{z}|\mathbf{x})$ that allows the use of the reparameterization trick. Although this yields a robust training regime, it limits the expressibility of the model to a subset of potential distributions. If instead we directly use the $g_\phi(\mathbf{x}, \epsilon)$ we can induce an arbitrarily complex posterior that would allow us to approximate any true posterior.\\
This idea has been recently realised using Gaussian processes by \cite{tran2015variational} who draw random latent input samples, push them through a non-linear mapping and then draw posterior samples. If we instead were to use a MLP to model $g_\phi(\mathbf{x}, \epsilon)$ we can, theoretically, model arbitrary posteriors. The problem now is the ability to yield a differentiable distribution over latent space which can potentially be sampling multiple $g_\phi(\mathbf{x}, \epsilon_i)$ to approximate a distribution, and batching gradients over all samples. This is akin to a Monte Carlo estimate of the variational posterior.

\subsection{Scheduled VAEB}
One of the most popular approaches in the unsupervised learning using autoencoding structures is making the learned representation robust to partial corruption of the input pattern \cite{vincentDAE}. This also proved to be an effective step in pre-training of deep neural architectures. Moreover, this method can be extended where the network is trained with a schedule of gradually decreasing noise levels \cite{sutton}.  This approach is motivated by a desire to encourage the network to learn a more diverse set of features from a coarse-grained to fine-grained ones.

Recently there was an effort to inject noise into both in input and in the stochastic hidden layer (denoising variational autoencoder, DVAE) which yields better score in terms of optimising the log likelihood \cite{bengioDVAE}. In order to estimate the variational lower bound the corrupted input $\mathbf{\widetilde{x}}$,  obtained from a known corruption distribution $p (\mathbf{\widetilde{x}} | \mathbf{x})$ around $\mathbf{x}$, requires to be integrated out which is intractable in the case of $\text{E}_{p (\mathbf{\widetilde{x}} | \mathbf{x})} \left[ q_{\phi}(\mathbf{z} | \mathbf{\widetilde{x}} )\right]$. Thus, Im et al. arrived at the new form of the objective function -- the denoising variational lower bound:

$$\mathcal{L}^{C} ~ \myeq ~ \text{E}_{\tilde{q}_\phi (\mathbf{z} | \mathbf{x})} \left[ 
\log \frac{p_{\theta} (\mathbf{x, z})}{q_\phi (\mathbf{z} | \mathbf{\tilde{x}})}
\right], $$

where $\tilde{q}_\phi (\mathbf{z} | \mathbf{x}) = \int q_\phi (\mathbf{z}| \mathbf{\tilde{x}}) p(\mathbf{\tilde{x}} | \mathbf{x}) \text{d}\mathbf{\tilde{x}}$. 

However, the noise in this case was set toa  constant during the training procedure. To the best of our knowledge no one analysed how the scheduling scheme might influence the learning of the auto-encoder's structure as well as the approximate form of the posterior of the latent variable. We believe that combination of both scheduled denoising training with variational form of auto-encoder should leads to gains in terms of the optimising lower bound and improving the reconstruction error as it was the case in the section $?$.



\end{document}