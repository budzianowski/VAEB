\documentclass[../report.tex]{subfiles}
\begin{document}
The aim of variational inference is to provide a deterministic approximation to an intractable posterior distribution by finding parameters $\phi$ such that $\DivKL{\Func{q_\phi}{\mathbf{\theta}}}{\Cond{\mathbf{\theta}}{D}}$ is minimised. This is achieved by noting that
\begin{align}
  \DivKL{\Func{q_\phi}{\mathbf{\theta}}}{\Cond{\mathbf{\theta}}{D}} =& \log \Prob{D} + \Expect{\Func{q_\phi}{\mathbf{\theta}}}{\log \Func{q_\phi}{\mathbf{\theta}}  - \log \Prob{\mathbf{\theta} ,D}} \nonumber \\
  =:& \log \Prob{X} - \Func{\mathcal{L}}{\phi; D}.
\end{align}
Noting that $\log \Prob{D}$ is constant w.r.t. $\phi$, we can now minimise the KL-divergence by maximising the evidence lower bound (ELBO) $\mathcal{L}$ (that this is indeed a lower bound follows from the non-negativity of the KL-divergence). Aside from some notable exceptions (eg. \cite{titsias2009variational}) this quantity is not tractably point-wise evaluable. However, if $\Func{q_\phi}{\mathbf{\theta}}$ and $\log \Prob{\mathbf{\theta}, D}$ are point-wise evaluable, it can be approximated using Monte Carlo as
\begin{equation}
  \Func{\mathcal{L}}{\phi; D} \approx \frac{1}{L} \sum_{l=1}^{L} \log \Prob{\mathbf{\theta}_l ,D} - \log \Func{q_\phi}{\mathbf{\theta}_l}, \quad \mathbf{\theta}_l \sim \Func{q_\phi}{\mathbf{\theta}}
\end{equation}
This stochastic approximation to the ELBO is not differentiable w.r.t. $\phi$ as the distribution from which each $\mathbf{\theta}_l$ is sampled itself depends upon $\phi$, meaning that the gradient of the log likelihood cannot be exploited to perform inference. One of the primary contributions of the paper being reviewed is to provide a differentiable estimator for $\mathcal{L}$ that allows gradient information in the log likelihood $\CondFunc{p_\theta}{\mathbf{x}_i}{\mathbf{z}_i}$ to be exploited, resulting in an estimator with lower variance. In particular it notes that if there exists a tractable reparameterisation of the random variable $\tilde{\mathbf{\theta}} \sim \Func{q_\phi}{\mathbf{\theta}}$ such that
\begin{equation}
  \tilde{\mathbf{\theta}} = \Func{g_\phi}{\epsilon}, \quad \epsilon \sim \Prob{\epsilon},
\end{equation}
then we can approximate the gradient of the ELBO as
\begin{equation}
  \Func{\mathcal{L}}{\phi; D} = \Expect{\Prob{\epsilon}}{\log \Prob{\mathbf{\theta} ,X} - \Func{q_\phi}{\mathbf{\theta}}} \approx \frac{1}{L} \sum_{l=1}^{L} \log \Prob{\mathbf{\theta}_l ,X} - \log \Func{q_\phi}{\mathbf{\theta}_l} =: \Func{\tilde{\mathcal{L}}^1}{\phi; X},
\end{equation}
where $\mathbf{\theta}_l = \Func{g_\phi}{\epsilon_l}$ and $\epsilon_l \sim \Prob{\epsilon}$. Thus the dependence of the sampled parameters $\mathbf{\theta}$ on $\phi$ has been removed, yielding a differentiable estimator provided that both $q_\phi$ and $\log \Prob{\mathbf{\theta} ,D}$ are themselves differentiable. Approximate inference can now be performed by computing the gradient of $\tilde{\mathcal{L}}^1$ w.r.t. $\phi$ either by hand or using one's favourite reverse-mode automatic differentiation package (eg. Autograd \cite{maclaurinautograd}) and performing gradient-based stochastic optimisation to maximise the elbo using, for example, AdaGrad \cite{duchi2011adaptive}.

The authors also point out that one can re-express the elbo in the following manner
\begin{equation}
  \Func{\mathcal{L}}{\phi; D} = \Expect{\Func{q_\phi}{\mathbf{\theta}}}{\log \Cond{D}{\mathbf{\theta}}} - \DivKL{\Func{q_\phi}{\mathbf{\theta}}}{\Prob{\mathbf{\theta}}}.
\end{equation}
This is useful as the KL-divergence between the variational approximation $\Func{q_\phi}{\mathbf{\theta}}$ and the prior over the parameters $\mathbf{\theta}$ has a tractable closed-form expression in a number of useful cases. This leads to a second estimator for the elbo:
\begin{equation}
  \Func{\tilde{\mathcal{L}}^2}{\phi; D} := \frac{1}{L} \sum_{l=1}^{L} \log \Cond{D}{\mathbf{\theta}_l} - \DivKL{\Func{q_\phi}{\mathbf{\theta}}}{\Prob{\mathbf{\theta}}}.
\end{equation}
It seems probable that this estimator will in general have lower variance than $\tilde{\mathcal{L}}^1$.

So far Stochastic Variational Inference has been discussed only in a general parametric setting. The paper's other primary contribution is to use a differentiable recognition network to learn to parameterise the posterior distribution over latent variables $z_i$ local to each observation $x_i$ in a parametric model. In particular, they assume that given some global parameters $\mathbf{\theta}$, $\mathbf{z}_i \sim \Func{p_\mathbf{\theta}}{\mathbf{z}}$ and $\mathbf{x}_i \sim \CondFunc{p_\mathbf{\theta}}{\mathbf{x}_i}{\mathbf{z}_i}$. In the general case the posterior distribution over each $z_i$ will be intractable. Furthermore, the number of latent variables $\mathbf{z}_i$ increases as the number of observations increases, meaning that under the framework discussed above we would have to optimise the variational objective with respect to each of them independently. This is potentially computationally intensive and quite wasteful as it completely disregards any information about the posterior distribution over the $z_i$ provided by the similarities between inputs locations $\mathbf{x}_{\neq i}$and corresponding posteriors $\mathbf{z}_{\neq i}$. To rectify this the recognition model $\CondFunc{q_\phi}{\mathbf{z}}{\mathbf{x}}$ is introduced.

Given the recognition model and a point estimate for $\mathbf{\theta}$, the ELBO becomes
\begin{align}
  \Func{\mathcal{L}}{\mathbf{\theta}, \phi; D} =& \,\, \Expect{\Func{q_\phi}{\mathbf{z_1}},...,\Func{q_\phi}{\mathbf{z}_N}}{\log \prod_{i=1}^{N} \Func{p_\mathbf{\theta}}{\mathbf{x}_i, \mathbf{z}_i} - \log \prod_{i=1}^{N} \Func{q_\phi}{\mathbf{z}}_i} \nonumber \\
  =& \sum_{i=1}^{N} \Expect{\Func{q_\phi}{\mathbf{z}_i}}{\log \Func{p_\mathbf{\theta}}{\mathbf{x}_i, \mathbf{z}_i} - \log \Func{q_\phi}{\mathbf{z}_i}} \nonumber \\
\end{align}

For this ELBO we can derive a similar result to $\tilde{\mathcal{L}}^1$, where we do not assume a closed-form solution for the KL divergence between distributions and include mini-batching to obtain an estimator for the ELBO for a mini-batch of observations
\begin{equation}
  \Func{\tilde{\mathcal{L}}^A}{\mathbf{\theta}, \phi; D} \approx \frac{N}{LM} \sum_{i=1}^{M} \sum_{l=1}^{L} \log \Func{p_\mathbf{\theta}}{\mathbf{x}_i, \mathbf{z}_{i,l}} - \log \Func{q_\phi}{\mathbf{z}_{i,l}}, \quad \mathbf{z}_{i,l} = \Func{g_\phi}{\mathbf{x}_i, \mathbf{\epsilon}_{i,l}},\,\, \mathbf{\epsilon}_{i,l} \sim \Func{p}{\mathbf{\epsilon}}
\end{equation}
where the $M$ observations in the mini-batch are drawn uniformly from the data set comprised of $N$ observations and for each observation we draw $L$ samples from the approximate posterior $\CondFunc{q_\phi}{\mathbf{z}_i}{\mathbf{x}_i}$.
Similarly, if $\CondFunc{q_\phi}{\mathbf{z}}{\mathbf{x}}$ and $\Func{p_\mathbf{\theta}}{\mathbf{z}}$ are such that the KL-divergence between them has a tractable closed-form solution then we can use an approximate bound which we could reasonably expect to have a lower variance:
\begin{align}
  \Func{\mathcal{L}}{\mathbf{\theta}, \phi; D} =& \,\,\Expect{\Func{q_\phi}{\mathbf{z}_1},...,\Func{q_\phi}{\mathbf{z}_N}}{\log \prod_{i=1}^{N} \CondFunc{p_\mathbf{\theta}}{\mathbf{x}_i}{\mathbf{z}_i}} - \DivKL{\prod_{i=1}^{N} \CondFunc{q_\phi}{\mathbf{z}_i}{\mathbf{x}_i}}{\prod_{i=1}^{N} \Func{p_\mathbf{\theta}}{\mathbf{z}_i}} \nonumber \\
  =& \sum_{i=1}^{N} \Expect{\CondFunc{q_\phi}{\mathbf{z}_i}{\mathbf{x}_i}}{\log \CondFunc{p_\mathbf{\theta}}{\mathbf{x}_i}{\mathbf{z}_i}} - \DivKL{\CondFunc{q_\phi}{\mathbf{z}_i}{\mathbf{x}_i}}{\Func{p_\mathbf{\theta}}{\mathbf{z}_i}} \nonumber \\
  \approx& \frac{N}{M} \sum_{i=1}^{M} \left[ \frac{1}{L} \sum_{l=1}^{L} \log \CondFunc{p_\mathbf{\theta}}{\mathbf{x}_i}{\mathbf{z}_{i,l}} - \DivKL{\CondFunc{q_{\phi}}{\mathbf{z}_i}{\mathbf{x}_i}}{\Func{p_\mathbf{\theta}}{\mathbf{z}_i}} \right] =: \Func{\tilde{\mathcal{L}}^B}{\mathbf{\theta}, \phi; D}, \label{eqn:Lb}
\end{align}
where $z_{i,l} = \Func{g_\phi}{\mathbf{x}_i, \mathbf{\epsilon}_{i,l}}$ and $\mathbf{\epsilon}_{i,l} \sim \Func{p}{\mathbf{\epsilon}}$.

\end{document}
